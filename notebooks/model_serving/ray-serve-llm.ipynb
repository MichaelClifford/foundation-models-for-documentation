{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118ffbf-10fb-4f1f-94b6-588470a574d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.ray.io/en/latest/serve/develop-and-deploy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c191d2-c444-421d-8ea3-c72cd4bd22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from starlette.requests import Request\n",
    "from typing import Dict\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "from fastapi import FastAPI\n",
    "\n",
    "from codeflare_sdk.cluster.cluster import Cluster, ClusterConfiguration\n",
    "from codeflare_sdk.cluster.auth import TokenAuthentication\n",
    "from codeflare_sdk.utils import generate_cert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee33b69-93a9-4112-8a74-2392dc188e45",
   "metadata": {},
   "source": [
    "### Serve a Flan T5 model using codeflare and instascale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d688b7f-0897-4639-9fe2-2d85121ed4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure our cluster object (and appwrapper)\n",
    "cluster = Cluster(ClusterConfiguration(\n",
    "    name='rayservice',\n",
    "    namespace='default',\n",
    "    num_workers=2,\n",
    "    min_cpus=2,\n",
    "    max_cpus=2,\n",
    "    min_memory=8,\n",
    "    max_memory=8,\n",
    "    image=\"quay.io/project-codeflare/ray:2.5.0-py38-cu116\",\n",
    "    num_gpus=1,\n",
    "    instascale=True,\n",
    "    machine_types=[\"m5.xlarge\", \"g4dn.xlarge\"] \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5b71e-7a5c-4cdd-bfcd-3e158bba7f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: before running cluster.up() you need to manually add the container port 8000 field to the raytest.yaml\n",
    "#    ports:\n",
    "#    - containerPort: 8000\n",
    "#    name: serve\n",
    "\n",
    "cluster.up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2669c-94ce-4fe6-a80a-e427b7086aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: using instascale=True in your ClusterConfiguration above assumes\n",
    "# that you have instascale properly installed and enabled on your cluster.\n",
    "# It can take around 15 minutes for your pods to scale up. \n",
    "# if this hangs for too long, please stop it with `cluster.down()`\n",
    "cluster.wait_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e77463-83bc-4e93-8012-223da16d07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_cluster_uri = cluster.cluster_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f0a23-4896-4e7e-b201-5ae639538960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install additionall libraries that will be required for model serving\n",
    "runtime_env = {\"pip\": [\"transformers\", \"datasets\", \"evaluate\", \"pyarrow<7.0.0\", \"accelerate\"]}\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "ray.init(address=ray_cluster_uri, runtime_env=runtime_env)\n",
    "\n",
    "print(\"Ray cluster is up and running: \", ray.is_initialized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89f973-3955-47b8-8281-09ef7db32d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Wrap the pretrained  flan-t5 instruction model in a Serve deployment.\n",
    "@serve.deployment(num_replicas=2, ray_actor_options={\"num_gpus\":1})\n",
    "#@serve.ingress(app)\n",
    "class SentimentAnalysisDeployment:\n",
    "    def __init__(self):\n",
    "        self._model = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\", device_map=\"auto\")\n",
    "\n",
    "    def __call__(self, request: Request) -> Dict:\n",
    "        return self._model(request.query_params[\"text\"])[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8636a9-73bb-4368-9a73-151ce3244a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Deploy the deployment.\n",
    "serve.run(SentimentAnalysisDeployment.bind(), host=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f0cdb-4117-4741-94ad-80f01701b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.get_deployment(\"default_SentimentAnalysisDeployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea7a6a-fe3e-46a6-9c02-a7668e994783",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.cluster_dashboard_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48681a7-023e-4dad-a12d-5954ea671575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Query the deployment and print the result from inside the cluster.\n",
    "requests.get(\"http://rayservice-head-svc.default.svc.cluster.local:8000/\", \n",
    "              params={\"text\": \"What is the purpose of AI?\"}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d553daa-b568-47a1-adba-8f7715f90f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Query the deployment and print the result from an exposed route.\n",
    "# an Openshift Route called ray-service must be created for this to work\n",
    "requests.post(\"http://ray-service-default.<CLUSTER_ADDRESS>\", \n",
    "              params={\"text\": \"What is the purpose of AI?\"}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6672c86a-a742-4105-a402-e2f5e24788f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b751f-410f-4224-a428-f5fc428610c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.down()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30294926-2f77-46aa-84c7-8f2404d8ef53",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Serve a LLAMA 2 model without instascale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce26f1b-9f65-43dc-a2c7-528394525a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oc login to cluster\n",
    "! oc whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f191936-4b0c-4abc-b354-4c979ad486f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! oc apply -f llama2-7b-ray.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c0b832-5d35-4a62-9700-192c2375d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install additional libraries that will be required for model serving\n",
    "runtime_env = {\"pip\": [\"transformers\", \"datasets\", \"evaluate\", \"pyarrow<7.0.0\", \"accelerate\"]}\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "ray.init(address=\"ray://test-llama2-head-svc.default.svc:10001\", runtime_env=runtime_env)\n",
    "\n",
    "print(\"Ray cluster is up and running: \", ray.is_initialized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82c451-6c01-4764-99fa-67fca903b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytoken= <INSERT-HUGGINGFACE-LLAMA2-MODEL-TOKEN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659771aa-9a14-499a-bd40-89cc4a54e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Wrap the pretrained  LLAMA2 instruction model in a Serve deployment.\n",
    "@serve.deployment(num_replicas=1, ray_actor_options={\"num_gpus\":1})\n",
    "#@serve.ingress(app)\n",
    "class RayServeDeployment:\n",
    "    def __init__(self):\n",
    "        self._model = pipeline(\"text2text-generation\", model=\"meta-llama/Llama-2-7b-hf\", device_map=\"auto\", token=mytoken)\n",
    "\n",
    "    def __call__(self, request: Request) -> Dict:\n",
    "        return self._model(request.query_params[\"text\"])[0]\n",
    "    \n",
    "    \n",
    "# 2: Deploy the deployment.\n",
    "serve.run(RayServeDeployment.bind(), host=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453344c-5283-464d-8364-2d0440ca8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.get_deployment(\"default_RayServeDeployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1ca8b-7335-44df-bf0a-29108991accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Query the deployment and print the result from inside the cluster.\n",
    "requests.get(\"http://test-llama2-head-svc.default.svc.cluster.local:8000/\", \n",
    "              params={\"text\": \"What is the purpose of AI?\"}).content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
