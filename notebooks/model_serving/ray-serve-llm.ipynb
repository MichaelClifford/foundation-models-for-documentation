{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118ffbf-10fb-4f1f-94b6-588470a574d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.ray.io/en/latest/serve/develop-and-deploy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c191d2-c444-421d-8ea3-c72cd4bd22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from starlette.requests import Request\n",
    "from typing import Dict\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "from fastapi import FastAPI\n",
    "\n",
    "from codeflare_sdk.cluster.cluster import Cluster, ClusterConfiguration\n",
    "from codeflare_sdk.cluster.auth import TokenAuthentication\n",
    "from codeflare_sdk.utils import generate_cert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d688b7f-0897-4639-9fe2-2d85121ed4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to: raytest.yaml\n"
     ]
    }
   ],
   "source": [
    "# Create and configure our cluster object (and appwrapper)\n",
    "cluster = Cluster(ClusterConfiguration(\n",
    "    name='raytest',\n",
    "    namespace='default',\n",
    "    num_workers=2,\n",
    "    min_cpus=2,\n",
    "    max_cpus=2,\n",
    "    min_memory=4,\n",
    "    max_memory=4,\n",
    "    image=\"quay.io/project-codeflare/ray:2.5.0-py38-cu116\",\n",
    "    num_gpus=0,\n",
    "    instascale=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a5b71e-7a5c-4cdd-bfcd-3e158bba7f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: before running cluster.up() you need to manually add the container port 8000 field to the raytest.yaml\n",
    "cluster.up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cec2669c-94ce-4fe6-a80a-e427b7086aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for requested resources to be set up...\n",
      "Requested cluster up and running!\n"
     ]
    }
   ],
   "source": [
    "cluster.wait_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e77463-83bc-4e93-8012-223da16d07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_cluster_uri = cluster.cluster_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c29f0a23-4896-4e7e-b201-5ae639538960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray cluster is up and running:  True\n"
     ]
    }
   ],
   "source": [
    "#install additionall libraries that will be required for model serving\n",
    "runtime_env = {\"pip\": [\"transformers\", \"datasets\", \"evaluate\", \"pyarrow<7.0.0\", \"accelerate\"]}\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "ray.init(address=ray_cluster_uri, runtime_env=runtime_env)\n",
    "\n",
    "print(\"Ray cluster is up and running: \", ray.is_initialized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd89f973-3955-47b8-8281-09ef7db32d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Wrap the pretrained sentiment analysis model in a Serve deployment.\n",
    "@serve.deployment(num_replicas=2)\n",
    "#@serve.ingress(app)\n",
    "class SentimentAnalysisDeployment:\n",
    "    def __init__(self):\n",
    "        self._model = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "    def __call__(self, request: Request) -> Dict:\n",
    "        return self._model(request.query_params[\"text\"])[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e8636a9-73bb-4368-9a73-151ce3244a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=372, ip=10.128.15.147)\u001b[0m INFO:     Started server process [372]\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=239, ip=10.128.12.42)\u001b[0m INFO:     Started server process [239]\n",
      "\u001b[2m\u001b[36m(ServeController pid=324, ip=10.128.15.147)\u001b[0m INFO 2023-08-16 07:33:08,503 controller 324 deployment_state.py:1298 - Deploying new version of deployment default_SentimentAnalysisDeployment.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=601)\u001b[0m INFO:     Started server process [601]\n",
      "\u001b[2m\u001b[36m(ServeController pid=324, ip=10.128.15.147)\u001b[0m INFO 2023-08-16 07:33:08,536 controller 324 deployment_state.py:1537 - Adding 2 replicas to deployment default_SentimentAnalysisDeployment.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 434kB/s]\n",
      "Downloading pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]\n",
      "Downloading pytorch_model.bin:  14%|█▎        | 41.9M/308M [00:00<00:00, 330MB/s]\n",
      "Downloading pytorch_model.bin:  27%|██▋       | 83.9M/308M [00:00<00:00, 368MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 427kB/s] \n",
      "Downloading pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s].128.12.42)\u001b[0m \n",
      "Downloading pytorch_model.bin:  41%|████      | 126M/308M [00:00<00:00, 332MB/s] \n",
      "Downloading pytorch_model.bin:  10%|█         | 31.5M/308M [00:00<00:00, 291MB/s]42)\u001b[0m \n",
      "Downloading pytorch_model.bin:  54%|█████▍    | 168M/308M [00:00<00:00, 327MB/s]\n",
      "Downloading pytorch_model.bin:  27%|██▋       | 83.9M/308M [00:00<00:00, 414MB/s]42)\u001b[0m \n",
      "Downloading pytorch_model.bin:  68%|██████▊   | 210M/308M [00:00<00:00, 323MB/s]\n",
      "Downloading pytorch_model.bin:  41%|████      | 126M/308M [00:00<00:00, 361MB/s] 42)\u001b[0m \n",
      "Downloading pytorch_model.bin:  82%|████████▏ | 252M/308M [00:00<00:00, 318MB/s]\n",
      "Downloading pytorch_model.bin:  54%|█████▍    | 168M/308M [00:00<00:00, 347MB/s].42)\u001b[0m \n",
      "Downloading pytorch_model.bin: 100%|██████████| 308M/308M [00:00<00:00, 340MB/s]\n",
      "Downloading pytorch_model.bin:  68%|██████▊   | 210M/308M [00:00<00:00, 362MB/s].42)\u001b[0m \n",
      "Downloading pytorch_model.bin:  85%|████████▌ | 262M/308M [00:00<00:00, 390MB/s].42)\u001b[0m \n",
      "Downloading pytorch_model.bin: 100%|██████████| 308M/308M [00:00<00:00, 383MB/s].42)\u001b[0m \n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 23.5kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 1.56MB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 288MB/s]\n",
      "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 69.4MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 1.38MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 46.4kB/s][0m \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 1.59MB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 278MB/s]28.12.42)\u001b[0m \n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 36.8MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 1.36MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayServeSyncHandle(deployment='default_SentimentAnalysisDeployment')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2: Deploy the deployment.\n",
    "serve.run(SentimentAnalysisDeployment.bind(), host=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "363f0cdb-4117-4741-94ad-80f01701b8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deployment(name=default_SentimentAnalysisDeployment,version=None,route_prefix=/)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.get_deployment(\"default_SentimentAnalysisDeployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a341436f-d45f-4ff3-94f5-a278d473b55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"generated_text\": \"to provide information to the user\"}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:default_SentimentAnalysisDeployment pid=283, ip=10.128.12.42)\u001b[0m INFO 2023-08-16 07:34:41,378 default_SentimentAnalysisDeployment default_SentimentAnalysisDeployment#YaQhKq XbSLirzWRj / default replica.py:654 - __CALL__ OK 182.9ms\n"
     ]
    }
   ],
   "source": [
    "# 3: Query the deployment and print the result from inside the cluster.\n",
    "requests.get(\"http://raytest-head-svc.default.svc.cluster.local:8000/\", \n",
    "              params={\"text\": \"What is the purpose of AI?\"}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d553daa-b568-47a1-adba-8f7715f90f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"generated_text\": \"to provide information to the user\"}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:default_SentimentAnalysisDeployment pid=703)\u001b[0m INFO 2023-08-16 07:34:51,660 default_SentimentAnalysisDeployment default_SentimentAnalysisDeployment#ilapgU eJChAUEvGS / default replica.py:654 - __CALL__ OK 182.0ms\n"
     ]
    }
   ],
   "source": [
    "# 3: Query the deployment and print the result from an exposed route.\n",
    "# an Openshift Route called ray-service must be made for this to work\n",
    "requests.post(\"http://ray-service-default.<CLUSTER_ADDRESS>\", \n",
    "              params={\"text\": \"What is the purpose of AI?\"}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6672c86a-a742-4105-a402-e2f5e24788f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=324, ip=10.128.15.147)\u001b[0m INFO 2023-08-16 07:35:07,128 controller 324 deployment_state.py:1264 - Deleting deployment default_SentimentAnalysisDeployment.\n",
      "\u001b[2m\u001b[36m(ServeController pid=324, ip=10.128.15.147)\u001b[0m INFO 2023-08-16 07:35:07,215 controller 324 deployment_state.py:1563 - Removing 2 replicas from deployment 'default_SentimentAnalysisDeployment'.\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "799b751f-410f-4224-a428-f5fc428610c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.down()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
